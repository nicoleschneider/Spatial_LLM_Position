\section{Experiments}
\label{section:experiments}

To support our position that further work is needed to enable LLMs to reason spatially, we first design a set of experiments (E1 - E5) to illustrate the present limitations of LLMs with regard to spatial reasoning.
The first set of experiments probes spatial reasoning ability across targeted spatial entity types, and the second set targets reasoning over various complex spatial relations.


\subsection{Spatial Entities} % -----------------------------------------------

\subsubsection{E1. Non-point Data}
\paragraph{Method}
\cite{Liu2023} can do NL2Spatial Query which can handle region/line data, but can an LLM handle it?
--> Repeat queries similar to \cite{Liu2023} nanjingtest and berlintest region and line queries but instead give them to ChatGPT.
% randomly pull non-overlapping rivers/highways as lines, lakes, seas, oceans as regions, and landmarks as points and construct directional queries about pairs of them: Lake A is to which side of landmark B

\subsubsection{E2. Lesser-known Cities}
To determine if LLMs are able to answer spatial questions about less populous locations and cities, we prompt Chat-GPT with the following question, filling in the blank with the name of a location in Australia.

\begin{center}
    \boxed{Where\ in\ the\ world\ is\ \langle location\ name\rangle ?}
\end{center}

The locations are selected based on their population size, ranging from \nrscomment{fill in details}
For each prompt, we measure whether the location was recognized or not, and whether the response is spatially accurate or not.

\paragraph{Result}



\subsection{Spatial Relations} % -----------------------------------------------

\subsubsection{E3. Multi-way Spatial Relations}
\paragraph{Method}
To determine if LLMs can reason about the spatial relationships between multiple locations, we randomly assign 18 Australian city names that Chapt-GPT recognized from Experiment E2 into six groups of three.
For each group consisting of cities A, B, and C, we query ChatGPT with prompt: 
\begin{center}
    \boxed{
    \!\begin{aligned}
    & A\ is\ north,\ northeast,\ northwest,\ south,\ southeast,\ \\
    & southwest,\ east,\ or\ west\ of\ B\ and\ C?
    \end{aligned}
    }
\end{center}

We repeat this for each permutation of A, B, and C within the prompt text, to construct a total of 36 multi-way spatial prompts.
% if needed: when wrong response is produce, the geo-coordinates are further included in the prompt.
% if needed: try the reverse where we ask for a city thats SE of A and NW of B
% if needed do more than 3 way and show how accuracy declines with more entities (more complexity)
% Test pairs as baseline? Should be similiar to results MaaSDB got

\paragraph{Result}
3/36 triples correct


\subsubsection{E4. Non-directional Spatial Relations}
\paragraph{Method}
To determine if LLMs can reason about the non-directional spatial relationships, we first test cyclic order relations by randomly assigning the top \nrscomment{n} most populous cities in Australia into \nrscomment{n/3} groups of three.
For each group consisting of cities A, B, and C, we query ChatGPT with prompt: A is counterclockwise or clockwise of B with respect to C.
\nrscomment{check this is faithful to cyclic order relations}

Next, we test topological relations by randomly selecting states and counties as regions (R), highways, roadways, and riverways as lines (L), and landmarks as points (P).
\osullikomment{Could be a really interesting twist using the first nation's territorial boundaries for this as a point of comparison. There are maps out there but they're not well known.}
Then we query Chat-GPT with prompt containing topological relations like: R contains or does not contain P, L1 intersects or does not intersect L2, R1 touches or does not touch R2. 

\osullikomment{This will be messy because some are easy - e.g. "Is Canberra in the ACT" -> yes. But there are some nasty ones too. The ACT is completely containsed in New South Wales, so if you asked "Is Canberra contained in New South Wales", transitively it would be yes, but technically it should be no...}

\paragraph{Result}


\subsubsection{E5. Spatiotemporal and Multiple Hop Relations}
\paragraph{Method}
--> Test if Chat-GPT can answer queries like
- whether event x happened north of event y (2 hops event->loc + loc->spatial)
- all events in x region that happened between y and z dates (intersect space and time)
- all events in x region that happened between y and z dates north of location A (spatiotemporal involving spatial relation) \osullikomment{I'd use GDELT data to give you a standard set of events. They're derived from newspaper articles so you should be able to limit to new events and get some confidence that they're not in the training data. Or NewsSTAND I guess....}

\paragraph{Result}








        
% Future Work -------------------------------------

% \subsection{SPM}
% \nrscomment{Experiment.}
% --> Try giving Chat-GPT a bunch of points A, B, C with coords like the pictorial query grid has and then asking it which cities in a given region match that pattern.
% Also try giving the input instead as a list of pairwise constraints.
% Use OSM to figure out the ground truth by pulling all city tags in the same region and running a traditional SPM algorithm to find all matching patterns.
% Report precision and recall for both input types.

% Check - can it produce an image from the points given? Can we give it an image with points as input?



% \paragraph{Locations being too close to differentiate in the embedding space}
% <ref from translation clustering paper>
% \nrscomment{Experiment.}
% --> Design tests to demonstrate these issues - similar to above, compare the embeddings of locations.