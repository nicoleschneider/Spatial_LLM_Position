
Recent work has demonstrated that large language models (LLMs) have some level of ``spatial awareness'' in the form of knowledge about geocoordinates, directional relationships between major cities, and relative distances between cities.
However. we show that LLMs cannot currently reason about complex spatial relationships like n-way directional relations, topological relations, and relations over complex forms of geospatial data like lines and regions.
%
We discuss the challenges associated with teaching LLMs to reason spatially, including the variety in structured and unstructured modalities of geodata, the concept of spatial heterogeneity, and the need for proper embedding methods to encode some types of geodata.
%
We present a vision for the future of neural spatial reasoning and provide several avenues to advance current LLMs towards achieving this vision.