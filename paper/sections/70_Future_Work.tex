\section{Future Work}
\label{section:future}

In this section we discuss future ways to improve the spatial reasoning abilities of LLMs, such as by explicitly devising embedding techniques and self-supervised training objectives that align with spatial tasks.

\subsection{Embedding Techniques}
We first suggest the development of novel embedding methods for various types of geodata.
To allow LLMs to learn complex spatial relations, such as multi-way directional relations and cyclic order relations that we showed to be a challenge for current LLMs, we propose tailoring an appropriate encoding scheme for those types of information.
In the spatial pattern matching domain, complex spatial relationships are captured explicitly using graph encodings, where relations can be described using the edges between graph nodes.
With the data in this format, spatial reasoning can be formulated directly as a graph reasoning task, which can be captured through existing heterogeneous graph learning objectives for problems like link prediction and node classification~\cite{Schneider2024}.
By adapting spatial problems to this framework, LLMs could be augmented with spatial reasoning abilities through the use of additional models or external tools that can capture explicit reasoning objectives that align with traditional spatial computing techniques.
Since graph data can be embedded using node-level, edge-level, community-level, and graph-level embeddings typical of Graph Neural Networks (GNNs)~\cite{Bai2019,Krlevza2016,Liu2020Neural}, this additional context at the node and community level and global context at the graph level may provide the LLM with adequate information to learn the spatial relationships captured in the data.



\subsection{Self-Supervised Training Tasks}
Once embedding methods have been well-established, self-supervised training objectives are needed to learn the embeddings during pre-training.
%
Some initial methods have been proposed for pre-training spatial coordinate embeddings that encode latitude and longitude coordinates for textual georeferences by crafting pseudosentences~\cite{Li2021}.
% In their paper, \citeauthor{Li2021} craft pseudosentences that list geoentities in descending order of distance to an entity of interest, and then employ self-supervised masked entity prediction and masked subtoken prediction objectives for pre-training.
To continue in that direction, more intuitive pre-training tasks can be developed that more thoroughly capture the topological, directional, and cyclic order spatial relationships that we highlight in this paper.
This could be done by leveraging natural language descriptions of the spatial relationships between neighboring entities including words like ``northeast of,'' ``adjacent to,'' ``left of,'' and ``clockwise of'' that might be applicable in downstream spatial reasoning tasks.

However, a mechanism for performing logical reasoning is still needed to resolve complex spatial questions.
To this end, work in learned indexing and neuro-symbolic reasoning may be combined with formal models of spatial logic like the nine-intersection model~\cite{Strobl2008}, which could offer a spatial grounding to the otherwise probabilistic nature of LLMs.
Self-supervised training objectives can be devised through the programmatic generation of training data capturing standard spatial properties, like spatial transitivity, that are needed to perform spatial reasoning.
To ensure self-consistent reasoning and further augment training data, training examples can be inverted, reversed, or negated where logic dictates that is appropriate.

For instance, the phrase ``A is northeast of B'' implies that ``B is southwest of A'' is also true.
By exposing the model to both forms of the spatial relationship, it may more readily learn the logical link between ``northeast'' and ``southwest''.
This technique can be applied to a variety of spatial relationships, including cyclic order relations and topological relations, which have fundamental properties of symmetry (``counterclockwise'' and ``clockwise,'' ``contains'' and ``contained by,'' etc.).
It could also be adapted to train the graph embeddings proposed earlier, where relations like ``northeast'' would be encoded as a link between A and B, and where spatial logic dictates, another link in the reverse direction could be automatically imputed.


\subsection{Multimodal Spatial Learning}
Once LLMs can be successfully pre-trained on various types of spatial data with success in downstream tasks, it will then be possible to leverage work in multimodal learning to combine a variety of input modalities of geospatial data.
Doing so would enable the training of a more generic geo-foundation model broadly capable of spatial reasoning given new sources of spatial information, at a variety of scales, in a variety of modalities.
To accomplish this goal, multimodal learning techniques that have been successful at enabling question answering over visual, textual, and other types of data could provide benefits in the spatial domain~\cite{Fei2022}.