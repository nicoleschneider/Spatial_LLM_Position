\section{Method}
\label{section:method}

In this section we describe our method for evaluating the spatial reasoning ability of LLMs.


\paragraph{Experimental Setup}
We devise a series of experiments to cover spatial reasoning questions in the form of toponym resolution, and four fundamental spatial relations: metric, directional, topological, and order relationships.

\nrscomment{@osullik insert table of models}

\nrscomment{@osullik describe data choices- sampling method, why AUS, where did list of places come from, ref library for reverse geocoding}

\nrscomment{gave city, state to help-- in all questions or just metric? either put it here or in the metric subsection, depending}

\nrscomment{introduce system-wide prompt}

\begin{lstlisting}[title=Prompt 1: Initial System Prompt]
    You are answering to evaluate spatial reasoning ability. You will be presented a question and asked to answer. Where there are multiple possible answers, select the most likely. Answer as briefly as possible, preferring single word answers where they suffice. Where you do not know the answer, it is unanswerable or you are uncertain, return 'ICATQ'.
\end{lstlisting}




\subsection{Experiment 1: Toponym Resolution}

\nrscomment{update with new format}

\begin{lstlisting}[title=Prompt 2: Toponym Resolution Prompt]
    Where in the world is A?
\end{lstlisting}

We use the results as a filter



\subsection{Experiment 2: Metric Relations}
To determine if LLMs can reason about metric (distance) spatial relations, we devise three types of prompts, each asking to retrieve places based on implicit quantitative metric relationships.
%
Specifically, given a reference pair of cities, $A$ and $B$ that are separated by a distance of $x$, and a query city $C$, the task is to retrieve a fourth city whose distance from $C$ is as close to $x$ as possible.
Since we aim to test for implicit metric reasoning, rather than stating the distance $x$ explicitly, we prompt the LLM for a city whose distance from $C$ is similar to the distance between $A$ and $B$.
The task is depicted visually in Figure \nrscomment{add ref to diagram}, where the ideal response will be the name of a city whose geocoordinates fall on or near the circle.
We use the following prompt to elicit responses from the models on this task:

\begin{lstlisting}[title=Prompt 3: Neutral Metric Prompt]
    The distance from A to B is similar to the distance from C to what other city or town?
\end{lstlisting}

\noindent Based on previous work~\cite{Bhandari2023}, LLMs have been shown to retrieve places that are closer to a query location when the prompt includes the keyword `near', and retrieve places that are farther from the query location when the keyword `far' is used in the prompt.
To further examine this idea we devise two variants of the neutral metric prompt above.
By introducing the keyword `near' or `far' into the prompt, we aim to measure if LLMs have a static representation of those keywords, or if they can adapt the meaning of `near' or `far' based on the scale of the question.
The `near' and `far' prompts are as follows:

\begin{lstlisting}[title=Prompt 4: `Near' Metric Prompt]
    If A is 'near' B, what is a city or town 'near' to C?
\end{lstlisting}

\begin{lstlisting}[title=Prompt 5: `Far' Metric Prompt]
    If A is 'far' B, what is a city or town 'far' to C?
\end{lstlisting}

\noindent We measure the distance between the place returned by the LLM and the query location $C$ and report this as \texttt{predicted{\_}distance}.
In the ideal case, the \texttt{predicted{\_}distance} will match closely to the distance $x$, which we report as the \texttt{target{\_}distance}.




\subsection{Experiment 3: Directional Relations}

\nrscomment{update numbers/construction method}

To determine if LLMs can reason about the spatial relationships between multiple locations, we randomly assign 18 Australian city names of varying population size~\footnote{Ranging from 5,297,089 to 37} into six groups of three.
For each group consisting of cities A, B, and C, we construct the following 3-way directional prompt: 

\begin{lstlisting}[title=Prompt 6: 3-way Directional Prompt]
    A is north, northeast, northwest, south, southeast, southwest, east, or west of B and C?
\end{lstlisting}

\nrscomment{test 2 way directional as baseline instead of relying on Qi2023}




We repeat this for each permutation of A, B, and C, reordering them within the prompt text, to construct a total of 36 three-way directional prompts.
% if needed: when wrong response is produce, the geo-coordinates are further included in the prompt.
% if needed: try the reverse where we ask for a city thats SE of A and NW of B
% if needed do more than 3 way and show how accuracy declines with more entities (more complexity)
% Test pairs as baseline? Should be similiar to results MaaSDB got



\subsection{Experiment 4: Topological Relations}

\nrscomment{update numbers/construction method}

To determine if LLMs can reason about topological relations, we select points (P) from a set of nine city names in Australia, regions (R) from a set of lakes, parks, regions, and states in Australia, and lines (L) from a set of highways, roadways, and riverways in Australia.
We construct prompts by selecting 18 pairs of point/line/region entities and assigning each pair a relation from a list of eight standard topological relations: \{\textit{equals, disjoint, intersects, touches, partially overlaps, within, contains}\}~\cite{Carniel2023}.
Using this set of relations, we construct the following topological relation prompts:

\begin{lstlisting}[title=Prompt 7: Topological Relation Prompt]
    ...
\end{lstlisting}

\nrscomment{prompting with definitions}

% To understand whether the model was indeed performing spatial reasoning on the topological prompts, we further prompted it with the reverse of some of the prompts.
% For instance, if the original prompt was ``Does R1 meet R2?'' we further prompted with ``Does R2 meet R1?'' and found that in several cases the response was correct for one prompt but not the other.
% These cases indicate that the errors we observe are due to failures in reasoning ability (i.e. that the spatial reasoning the model is doing is not self-consistent) rather than incorrect information about an entity's position in space (such as having false information that a city is located somewhere different from where it actually exists).
% In section \ref{section:future} we discuss data augmentation techniques that may help address the self-consistency issues observed here.



\subsection{Experiment 5: Cyclic Order Relations}

\nrscomment{update numbers/construction method}

To determine if LLMs can reason about cyclic order relationships, we randomly assign nine Australian city names into three groups of three.
For each group consisting of cities A, B, and C, we construct the following prompt: 

\begin{lstlisting}[title=Prompt 8: Cyclic Order Relation Prompt]
    With respect to a centroid in A, is moving from B to C a clockwise or counterclockwise direction?
\end{lstlisting}

We repeat this for each permutation of A, B, and C, reordering them within the prompt text, to construct a total of 18 three-way directional prompts.








% \begin{center}
%     \boxed{
%     \!\begin{aligned}
%     & A\ is\ north,\ northeast,\ northwest,\ south,\ southeast,\ \\
%     & southwest,\ east,\ or\ west\ of\ B\ and\ C?
%     \end{aligned}
%     }
% \end{center}







        
% Future Work -------------------------------------

% \subsubsection{E0.1 Non-point Data}
% \paragraph{Method}
% \cite{Liu2023} can do NL2Spatial Query which can handle region/line data, but can an LLM handle it?
% --> Repeat queries similar to \cite{Liu2023} nanjingtest and berlintest region and line queries but instead give them to ChatGPT.
% % randomly pull non-overlapping rivers/highways as lines, lakes, seas, oceans as regions, and landmarks as points and construct directional queries about pairs of them: Lake A is to which side of landmark B


% \nrscomment{Make this a one liner explaining the random selection process and testing to verify it recognizes the cities}
% \nrscomment{move this out to a paper on embeddings}

% \subsubsection{E0.2 Lesser-known Cities}
% To determine if LLMs are able to answer spatial questions about less populous locations and cities, we select 20 Australian city names with varying population sizes and prompt Chat-GPT with the following question, filling in \textit{L} with each city name.

% \begin{center}
%     \boxed{Where\ in\ the\ world\ is\ L?}
% \end{center}

% The locations are selected based on their population size, ranging from 5,297,089 to 37. \nrscomment{fill in details}
% For each prompt, we measure whether the location was recognized or not, and whether the response is spatially accurate or not. 


% \subsubsection{E5. Spatiotemporal and Multiple Hop Relations}
% \paragraph{Method}
% --> Test if Chat-GPT can answer queries like
% - whether event x happened north of event y (2 hops event->loc + loc->spatial)
% - all events in x region that happened between y and z dates (intersect space and time)
% - all events in x region that happened between y and z dates north of location A (spatiotemporal involving spatial relation) 
%\osullikomment{I'd use GDELT data to give you a standard set of events. They're derived from newspaper articles so you should be able to limit to new events and get some confidence that they're not in the training data. Or NewsSTAND I guess....}

%\osullikomment{Could be a really interesting twist using the first nation's territorial boundaries for this as a point of comparison. There are maps out there but they're not well known.}


% \subsection{SPM}
% \nrscomment{Experiment.}
% --> Try giving Chat-GPT a bunch of points A, B, C with coords like the pictorial query grid has and then asking it which cities in a given region match that pattern.
% Also try giving the input instead as a list of pairwise constraints.
% Use OSM to figure out the ground truth by pulling all city tags in the same region and running a traditional SPM algorithm to find all matching patterns.
% Report precision and recall for both input types.

% Check - can it produce an image from the points given? Can we give it an image with points as input?



% \paragraph{Locations being too close to differentiate in the embedding space}
% <ref from translation clustering paper>
% \nrscomment{Experiment.}
% --> Design tests to demonstrate these issues - similar to above, compare the embeddings of locations.