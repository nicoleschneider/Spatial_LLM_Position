\section{Method}
\label{section:method}

In this section we describe our method for evaluating the spatial reasoning ability of LLMs, detailing the models, evaluation metrics, and prompts used.


\subsection{Experimental Setup}
We devise a series of experiments to cover spatial reasoning questions about three fundamental spatial relations that have been under-studied in LLMs: directional, topological, and cyclic order relationships.
To isolate the effect of toponym ambiguity on the results, we also devise a set of toponym resolution questions for the place names that appear in the spatial reasoning questions.

\input{paper/tables/dataset_design}

\subsubsection{Dataset.} 
We develop our own evaluation dataset for two reasons: (i) there are no existing benchmarks that cover directional, topological, and cyclic order relationships about geoentities (as opposed to fictitious entities, which are less useful to reason about in many geospatial applications and do not allow LLMs to use their world knowledge to help answer the question) and (ii) to reduce the likelihood that the specific questions we ask have been encountered during pre-training by the models under test. 
We select Australia as the broad region of test because it is an English-speaking country with common place names that are likely to be in the vocabulary of the models, as well as Indigenous place names that are less likely to be encountered frequently during training. 
Our dataset contains $179$ questions that cover the \textit{point, line} and \textit{region} spatial entities and \textit{directional, topological} and \textit{cyclic order} relations commonly used in spatial pattern matching and spatial reasoning problems.
Details about the dataset questions can be found in Table \ref{tab:dataset_design}.




\subsubsection{Evaluation Metrics}
We follow \citeauthor{Feng2024}~\cite{Feng2024} and evaluate correctness of LLM responses against hand-labeled ground truth answers using the following accuracy metrics, which account for correct, incorrect, and abstain responses, which we represent using the symbols outlined below.

\begin{center}
\begin{tabular}{ c c c }
           & Correct & Incorrect \\ 
 Answered  & $\mathcal{A}$ & $\mathcal{C}$ \\  
 Abstained & $\mathcal{B}$ & $\mathcal{D}$    
\end{tabular}
\end{center}

\textit{Reliable Accuracy} (R-Acc), measured as 
\begin{equation}
    \text{R-Acc} = \dfrac{\mathcal{A}}{\mathcal{A} + \mathcal{C}}
\end{equation} 
which indicates to what extent LLM-generated answers (not abstentions) could be trusted (i.e., out of all questions answered, how many are correct?)~\cite{Feng2024}. 

\textit{Effective Reliability} (ER), measured as
\begin{equation}
    \text{ER} = \dfrac{\mathcal{A} - \mathcal{C}}{\mathcal{A} + \mathcal{B} + \mathcal{C} + \mathcal{D}}
\end{equation} 
which strikes a balance between reliability and coverage (i.e., out of all questions, how many more are answered correctly than incorrectly?)~\cite{Feng2024}

\textit{Abstention accuracy} (A-Acc), measured as
\begin{equation}
    \text{A-Acc} = \dfrac{\mathcal{A} + \mathcal{D}}{\mathcal{A} + \mathcal{B} + \mathcal{C} + \mathcal{D}}
\end{equation} 
which evaluates whether the abstain decisions are correct (i.e. did the LLM abstain when it would provide an incorrect answer and vice versa?)~\cite{Feng2024}. 



\subsubsection{Models.}
We select $14$ LLMs from five of the leading LLM developers. 
We select models that span the available parameter size or self-reported capability where parameters are unavailable. 
For experimentation, temperature is set to $0$ on each model and where available, constant seed values are set to reduce the impact of randomness in generation.

\nrscomment{describe the RAG variations used and what the knowledge store is for each spatial relation that they have access to}

\subsubsection{Prompting.} 
Each question is provided as an isolated prompt to the model under test so no context is carried between interactions.
We use a zero-shot prompting approach and only provide retrieved spatial context for the RAG settings.
For each experiment, we begin by providing an initial prompt stating the goal of evaluating spatial reasoning, as follows:

\begin{lstlisting}[title=Prompt 1: Initial System Prompt]
 You are answering to evaluate spatial reasoning ability. 
 You will be presented a question and asked to answer. 
 Where there are multiple possible answers, select the 
 most likely. Answer as briefly as possible, 
 preferring single word answers where they suffice. 
 Where you do not know the answer, it is unanswerable 
 or you are uncertain, return 'ICATQ'.
\end{lstlisting}

\noindent The subsequent prompt is determined by the experiment being conducted.
For each experiment we describe below the specific prompt format used for the second prompt, with letters (A, B, C, etc.) as placeholders for spatial entities such as cities, rivers, roads, and states that we fill in to construct the final prompts.


\subsection{Experiment 1: Toponym Resolution}

Following \citeauthor{Osullivan2024}~\cite{Osullivan2024}, we test all points included in our dataset with toponym resolution to identify whether each term is associated with Australia.
For each place name used in subsequent experiments, we construct the following toponym resolution question, where A is replaced by the name of the point, line, or region of interest.

\begin{lstlisting}[title=Prompt 2: Toponym Resolution Prompt]
 Where is A? Format your answer as a comma seperated 
 list: state/county, country.
\end{lstlisting}

\noindent The goal of the toponym resolution questions is to identify terms strongly associated with other countries, (like `Roma' with `Italy' rather than the small town in the Maranoa Region of Queensland, Australia), to verify that the majority of the toponyms in subsequent questions are independently associated with Australia by the models being tested.
For each question we record whether the model responds correctly, incorrectly, or abstains from answering.



\subsection{Experiment 2: Directional Relationships}

To determine if LLMs can reason about the directional spatial relationship between locations, we construct a series of prompts asking about the cardinal directionality between geoentities.
We create $42$ queries covering $18$ Australian city and town names of varying population size~\footnote{Ranging from population of $37$ to $5,297,089$ } into groups of `2-way' and `3-way' constraint problems.
For each group consisting of entities A, B, and C, we construct the following 2-way and 3-way directional prompts, where A, B, and C are replaced with actual city or town names: 

\begin{lstlisting}[title=Prompt 3: 2-way Directional Prompt]
 A is north, northeast, northwest, south, southeast, 
 southwest, east, or west of B?
\end{lstlisting}

\begin{lstlisting}[title=Prompt 4: 3-way Directional Prompt]
 A is north, northeast, northwest, south, southeast, 
 southwest, east, or west of B and C?
\end{lstlisting}

\noindent We repeat this for each permutation of A, B, and C, reordering them within the prompt text.
For each question we record whether the model responds correctly, incorrectly, or abstains from answering.



\subsection{Experiment 3: Topological Relationships}
To determine if LLMs can reason about topological relations, we construct a series of seven different topological relation prompts containing questions pertaining to each of the major topological predicates~\cite{Carniel2023}.
We select points from a set of city and town names in Australia, regions from a set of lakes, parks, regions, and states in Australia, and lines from a set of highways, roadways, and riverways in Australia.
The prompts are structured as follows, where A and B are replaced with point, line, or region names as appropriate to the predicate being used:

\begin{lstlisting}[title=Prompts 5-11: Topological Relation Prompts]
 Is A geospatially equal to B?
 Is A geospatially disjoint from B?
 Does A geospatially intersect B?
 Does A geospatially touch B?
 Does A geospatially partially overlap B?
 Is A geospatially within B?
 Does A geospatially contain B?
\end{lstlisting}

\noindent We populate the prompts with cities and towns of varying populations, major waterways, and the `common name' for major roadways (i.e. "The Pacific Highway" rather than "M1 Motorway") sampled from across the states and territories of Australia.
For each question we record whether the model responds correctly, incorrectly, or abstains from answering.


\subsection{Experiment 4: Cyclic Order Relationships}
To determine if LLMs can reason about cyclic order relationships, we construct a series of prompts asking about the clockwise or counterclockwise relationship between entities.
For each group consisting of entities A, B, and C, we construct the following prompt, where A, B, and C are replaced with city names: 

\begin{lstlisting}[title=Prompt 12: Cyclic Order Relation Prompt]
 With respect to a centroid in A, is moving from B to C 
 a clockwise or counterclockwise direction?
\end{lstlisting}

\noindent 
% We permute the ordering of A, B, and C, within the prompt text to ensure the questions require performing correct cyclic order reasoning.
For each question we record whether the model responds correctly, incorrectly, or abstains from answering.
