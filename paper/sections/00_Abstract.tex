
Spatial reasoning is a particularly challenging form of reasoning that requires inferring implicit information about objects based on their relative positions in space.
Traditionally, spatial reasoning is addressed using formal methods that rely on pre-computed indices and data structures, which limit the scope of questions that can be answered.
As the research community moves towards developing general purpose geo-foundation models that can perform a variety of spatial reasoning tasks, recent research has explored what kinds of world knowledge and spatial reasoning capabilities Large Language Models (LLMs) naturally inherit from their training data.
However, much of the existing work focuses on a few models and evaluates them on factoid-based questions, Point of Interest (POI) recommendation tasks, and distance-based reasoning.
In this paper we assess the spatial reasoning ability of 14 different LLMs with and without Retrieval Augmented Generation (RAG) through a set of experiments that cover a broad range of spatial tasks, including reasoning about three fundamental spatial relationships that have largely been understudied in LLMs: topological, directional, and cyclic order relations.
We find that performance on some of these relation types, especially when complexity of the task includes more than two entities, reveals significant gaps in the spatial reasoning abilities of the LLMs tested.
Given these findings, we suggest several avenues of opportunity to improve the spatial reasoning ability of LLMs.
