\section{Conclusion}
\label{section:conclusion}
\normalsize

\nrscomment{clean this up now that experiments are filled out}

Recent work has demonstrated that large language models (LLMs) have some level of spatial awareness in the form of knowledge about geocoordinates, directional relationships between major cities, and relative distances between cities.
%
However. we show that LLMs cannot currently reason about complex spatial relationships like multi-way directional relations and cyclic order relations. %, and relations over complex forms of geospatial data like regions.
%
This paper presents the position that further work is needed before LLMs can infer implicit spatial information that they were not explicitly exposed to during training.
%
To support this position, we discuss the challenges associated with spatial reasoning, including the variety of modalities and scales of geodata.
%
Finally, we present a vision for the future of neural spatial reasoning and suggest several avenues of research to advance current LLM research towards achieving this vision, including devising new embedding methods and developing intuitive self-supervised training objectives for spatial data.