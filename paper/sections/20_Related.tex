\section{Related Work}
\label{section:related}
\normalsize

Many recent works have explored the abilities and limitations of LLMs for performing various reasoning tasks, from math word problems~\cite{Gao2023, Badaro2023} to place name resolution \cite{Mai2024}.
In this section we summarize the recent work investigating the degree to which LLMs can reason spatially.
We describe recent work probing LLMs for spatial and geographic knowledge, papers on geo-foundation models, and papers proposing techniques that enable LLMs to answer spatial questions.

\subsection{Spatial and Geographic Knowledge in LLMs}
A few recent works have probed to what degree LLMs like ChatGPT can answer questions involving spatial components.
\citeauthor{Mooney2023} reveal limitations in ChatGPT's fundamental GIS knowledge by prompting it with multiple choice questions from a GIS exam~\cite{Mooney2023}.
\citeauthor{Bang2023}~\cite{Bang2023} shows that ChatGPT is unreliable at general reasoning tasks, and
\citeauthor{Cohn2023}~\cite{Cohn2023} show that it fails at generic (non-geographic) spatial reasoning questions like whether a larger circle be contained inside a smaller one.
In more task-oriented exploration, 
\citeauthor{Xie2023translating} show that LLMs perform poorly at planning on tasks that involve spatial reasoning, including moving objects around for robotics applications~\cite{Xie2023translating}.
Similarly, 
\citeauthor{Tao2023} find faults in map visualizations and sketch maps generated by ChatGPT using code or ASCII symbols~\cite{Tao2023}.
These findings collectively reveal potential issues in the fundamental reasoning and geographic knowledge of LLMs, motivating our further exploration into reasoning about specific categories of spatial relationships.

\subsection{Geo-foundation Models}
Recent vision papers suggest that LLMs show promise for use as geospatial databases, where the LLM performs spatial reasoning given natural language prompts~\cite{Bhandari2023, Qi2023}.
\citeauthor{Bhandari2023}~\cite{Bhandari2023} and \citeauthor{Qi2023}~\cite{Qi2023} support their visions with experiments prompting LLMs for geocoordinates of popular cities, with results showing promisingly accurate coordinates for most of the cities tested.
\citeauthor{Bhandari2023} further prompt the LLMs to generate names of cities that are ``near'' or ``far'' from a provided reference city.
They show that the resulting cities generated tend to be closer in distance to the reference point when the ``near'' prompt is used, and farther when the ``far'' prompt is used, indicating LLMs are capable of basic spatial reasoning in that they associate distance meaning to those keywords.
However, other work has shown that LLMs lack the ability to adapt the meaning of those same keywords to the scale relevant to the question asked, when provided with reference points indicating what ``near'' or ``far'' means in a given context~\cite{Osullivan2024}.
RAG has been shown to improve distance-based reasoning in LLMs using techniques like DistRAG~\cite{Schneider2025b}, but no previous work has explored distance-invariant geospatial reasoning in LLMs, like topological or directional reasoning between geoentities, to our knowledge.


\subsection{LLM Adaptations for Geodata}
Several embedding methods and model architectures have been proposed to enable LLMs to handle certain types of geospatial data.
For trajectories of geocoordinates, \citeauthor{Hu2023} propose an embedding method that uses sub-trajectory similarity learning to pre-train trajectory representations that can be used in downstream prediction tasks~\cite{Hu2023}.
For textual georeferences, \citeauthor{Li2021} design a pre-training task using spatial coordinate embeddings %(based on latitude and longitude) 
corresponding to textual georeferences, which improve accuracy over non-spatial methods on geoentity type prediction and linking to knowledge graphs~\cite{Li2021}.
% They create pseudosentences consisting of a geoentity reference and neighboring entities, listed in increasing order of distance from the reference entity~\cite{Li2021}.
\citeauthor{Li2023b} design a contrastive learning method, GeoLM, that performs toponym recognition and linking, geoentity typing, and geospatial relation extraction~\cite{Li2023b}.
For spatio-temporal forecasting, \citeauthor{Liu2024large} propose a method of tokenization and encoding to increase LLM understanding of spatio-temporal text references~\cite{Liu2024large}.

Some other benchmarks and new methods have also been proposed to advance LLM performance on geospatial tasks, but the majority of these focus on a few specific task categories, like
factoid-based or population variable prediction questions~\cite{Qi2023,Roberts2023,Gupta2024,Yan2024,Manvi2024,Lietard2021}, 
trajectory prediction or POI and itinerary recommendation tasks~\cite{Schneider2025,Yu2025,Roberts2023,Xie2024,Gundawar2024,De2024,Sharma2023}, 
distance-based reasoning questions~\cite{Bhandari2023,Osullivan2024,Schneider2024b}, 
or visual tasks like geo-localization, geographic visualization, and traffic signal control~\cite{Feng2024b,Chen2024}.
None of these methods explicitly address topological, directional, or cyclic order relations over geographic entities, though some downstream questions inherently require reasoning about these relationships.
Examples include \citeauthor{Li2024} which address the StepGame Benchmark for directional reasoning over fictional entities labeled with letters in a scene~\cite{Li2024} and \citeauthor{Majic2024} which evaluate a brick stacking task~\cite{Majic2024}.

% We add to this landscape by showing how base LLMs and RAG variants perform on explicit spatial reasoning questions through the experiments in this paper and revealing that RAG is not sufficient to induce correct spatial reasoning.
We add to the research landscape by showing how unaugmented LLMs, and RAG LLM variants perform on explicit spatial reasoning questions. 
Through the experiments in this paper, we find that RAG using graphs of geodesic distances is not sufficient to induce correct spatial reasoning in isolation.
 