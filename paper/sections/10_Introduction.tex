\section{Introduction}
\label{section:introduction}

Humans possess extensive world knowledge, experience, spatial intuition, common sense, and embodiment, without which it would be impossible to navigate from one place to another, identify locations by their nearby landmarks, and avoid colliding with other moving entities.
While humans seamlessly apply spatial reasoning to the objects they perceive in their environment every day, it is actually quite a challenging form of reasoning that often requires dynamically gathering data and inferring implicit information from objects as they move through space.

Traditionally, many spatial reasoning tasks like spatial pattern matching (determining which objects in the world match a set of spatial constraints) can be addressed using formal methods applied to specific formats of data~\cite{Papadias1998, Schwering2014, Duckham2023,Folkers2000, Chen2019, Fang2019, Minervino2023, Osul2023, Osul2023b}.
However, these traditional methods are rigid and computationally slow, often relying on pre-computed indices and data structures, which limit the set of questions that they can answer.
While some work aims to apply machine learning as an approximate solution to specific spatial reasoning tasks like spatial pattern matching~\cite{Schneider2024, Schneider2024b}, the broader interest is moving towards developing flexible `geo-foundation models' that can perform a variety of spatial reasoning tasks inherently, using their geospatial world knowledge.

With the advent of Large Language Models (LLMs), recent research has explored what kinds of world knowledge and spatial reasoning capabilities current LLMs have inherited from their vast training data, as a potential avenue to achieving a geo-foundation model~\cite{Mai2024, Bhandari2023, Qi2023, Xie2023translating, Mooney2023,Cohn2023,Bang2023}.
Other approaches have attempted to augment LLMs with additional spatial information using Retrieval Augmented Generation (RAG)~\cite{Yu2025,Schneider2025b}. \nrscomment{cite journal}
These developments are critical given LLMs are being used for increasingly complex tasks, including ones grounded in the physical world, like generating routes between known points or suggesting places of interest to a user based on their location and trajectory~\cite{Schneider2025,Yu2025}.
Additionally, spatial data is extensively available from various sources, at various scales, including trajectory data, overhead imagery, crowd-sourced geotags, motion sensor output, and dashboard camera footage.

However, much of the existing work evaluating LLMs on spatial data focuses on a few models and evaluates them on factoid-based questions~\cite{Qi2023,Roberts2023,Gupta2024,Yan2024}, Point of Interest (POI) and itinerary recommendation tasks~\cite{Schneider2025,Yu2025,Roberts2023,Xie2024,Gundawar2024,De2024}, and distance-based reasoning questions~\cite{Bhandari2023,Osullivan2024,Schneider2025b} \nrscomment{cite journal}.
These works have shown that LLMs possess some basic level of ``spatial awareness'' in the form of knowledge about geocoordinates, distances between cities, and qualitative distance indicators like ``near'' and ``far''~\cite{Bhandari2023,Qi2023,Osullivan2024}.
However, taking the broader problem of spatial reasoning, it has previously been unclear to what degree general purpose LLMs are able to reason about other types of implicit spatial relationships, given their exposure to spatial information during training.

We address this gap by developing a set of spatial reasoning tasks that includes broad coverage of the major spatial relations, as well as complex questions that require reasoning about more than two spatial entities at a time.
We assess the spatial reasoning ability of 14 different LLMs with and without RAG through a set of experiments that cover a wide range of spatial tasks, including reasoning about three fundamental spatial relationships that have largely been understudied in LLMs: topological, directional, and cyclic order relations.
We find that performance on some of these relation types, especially when complexity of the task includes more than two entities, reveals significant gaps in the spatial reasoning abilities of the LLMs tested. 
We find that LLMs perform poorly, and cannot infer spatial information not explicitly stated in the question \nrscomment{or contradict themselves or whatever the key finding is, and compare how RAG helps or does not help}.
In particular, we find that current LLMs struggle to perform complex spatial reasoning about:

\begin{itemize}
    \item Directional relations between more than two entities, 
    \item Topological relations, especially `intersect', `partially overlap', and `within', and
    \item Order relations, generally.
\end{itemize}

Given these findings, we suggest several avenues of opportunity to improve the spatial reasoning ability of LLMs.
Specifically, we envision the development of new embedding methods and self-supervised learning objectives that can be used to enable LLMs to learn over complex spatial data.
We suggest leveraging recent work in multimodal learning~\cite{Xue2023, Trappolini2023} and symbolic reasoning~\cite{Lee2023} to combine different input modalities and train geo-foundation models using spatially relevant self-supervised training objectives that have the potential to generalize to a wide range of downstream spatial tasks.
To aid in this effort we release a dataset consisting of $179$ spatial prompts and corresponding ground truth responses, as well as a framework for querying 14 LLMs programmatically and scoring their responses,~\footnote{Code, Data and Results are available at: \url{https://github.com/nicoleschneider/Spatial_LLM_Position}} which will help researchers test future models on the spatial reasoning tasks we describe in this paper. 

\nrscomment{update git to separate from metric questions and last paper}


The rest of this paper is organized as follows.
In section \ref{section:related} we describe recent work assessing and improving the spatial knowledge of LLMs.
Then, section \ref{section:background} presents the necessary background in spatial reasoning.
Section \ref{section:method} outlines our experimental method and the design of our spatial reasoning dataset, followed by the experimental results in section \ref{section:results}, and a discussion of the significance in section \ref{section:discussion}.
Finally, in section \ref{section:future} we describe future work based on our findings, before concluding in section \ref{section:conclusion}. 
