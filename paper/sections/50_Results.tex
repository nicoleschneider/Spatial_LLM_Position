\section{Results}
\label{section:results}

In this section we discuss the results (R1 - R5) of the experiments E1 - E5 that were laid out in the previous section.


\subsection{Result 1. Toponym Resolution}



% \subsection{R2. Lesser-known Cities}
% --> Show correlation between cities recognized and their population size. Plot for each population value tested the binary value indicating if it was recognized or not by Chat-GPT.
% Give the phrase Chat-GPT outputs when a location is not recognized.
% Plot accuracy of result vs. population size.

%\osullikomment{I think you might get some intersting outliers. E.g. I think Port Arthur will be massively over-represented because it was the site of our last mass shooting, and the catalyst for gun control in Australia. So there are a million articles that talk about Port Arthur, even though it's tiny in real terms.}



\subsection{Result 2. Metric Relations}

Figure \ref{fig:metric-plots} shows the results of Experiment 2.
Across all three prompt types, the distances between the places returned by the models and the query points (\texttt{predicted{\_}distances)} were not similar to the distances between the reference places provided in the prompt (\texttt{target{\_}distances).

- off by thousands of kilometers in many caes
- keywords heavily biased responses toward smaller or larger distances, meaning the other cities given in the prompt were largeley ignored when those keywords were present
- especially for near- when near was present the city returned never exceeded 1000KM in distance from the query point, no matter how large the reference distance indicating the scale that near should be, up to 4000km

\begin{figure*}[h]
    \centering
    \subfigure[Target distance vs. predicted distance for `far' metric prompt. Three outliers with extremely high predicted distances are omitted, along with 100 abstentions out of 280 model responses.]{
        \includesvg[width=0.3\textwidth]{figures/metric_scatter_far}
        \label{fig:metric-plot-far}
    }
    \hfill
    \subfigure[Target distance vs. predicted distance for neutral metric prompt. Four outliers with extremely high predicted distances are omitted, along with 159 abstentions out of 280 model responses.]{
        \includesvg[width=0.3\textwidth]{figures/metric_scatter_neutral}
        \label{fig:metric-plot-neutral}
    }
    \hfill
    \subfigure[Target distance vs. predicted distance for `near' metric prompt. Eleven outliers with extremely high predicted distances are omitted, along with 89 abstentions out of 280 model responses.]{
        \includesvg[width=0.3\textwidth]{figures/metric_scatter_near}
        \label{fig:metric-plot-near}
    }
    \caption{Results of three metric relation prompts: `far', neutral, and `near'. Target distance represents the distance between `A' and `B' in Prompts 3-5, and predicted distance represents the distance between `C' and the place returned by the model. Points closer to the line drawn at $y = x$ represent more accurate metric spatial reasoning than points further from that line.}
    \label{fig:metric-plots}
\end{figure*}


% \begin{figure*}[h]
%     \centering
%     \begin{subfigure}[t]{\textwidth}
%         \includesvg[width=.25\textwidth]{figures/metric_scatter_far}
%         % \caption{\small A candidate location X has named objects A-D with the spatial layout depicted above.} 
%         \label{fig:CM-LO-Example}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{\textwidth}
%         \includesvg[width=.25\textwidth]{figures/metric_scatter_neutral}
%         % \caption{\small The objects are binned into spatial quadrants based on their relative position to the location coordinates, X.} 
%         \label{fig:CM-LO-Setup}
%     \end{subfigure}
%     \hfill
%         \begin{subfigure}[t]{\textwidth}
%         \includesvg[width=.25\textwidth]{figures/metric_scatter_near}
%         % \caption{\small Rank the locations by the number of query terms found in the correct quadrant for the location.}
%         \label{fig:CM-LO-Query}
%     \hfill
%     \end{subfigure}
%     \caption{\textbf{Location-Object Search Method. A Location-Object data structure (Figure \ref{fig:CM-LO-Setup}) is generated based on the cardinal relations between the objects and the location (Figure \ref{fig:CM-LO-Example}). Then a pictorial query is matched against the structure (Figure \ref{fig:CM-LO-Query}).}}\label{figure:ConceptMap-LO} 
% \end{figure*}


\subsection{Result 3. Directional Relations}
The returned results for the 3-way directional prompts were correct in 3 out of 36 instances.
For comparison, \citeauthor{Qi2023} performed pairwise directional prompting for cities in Australia and found the responses to be correct in 44 out of 50 cases~\cite{Qi2023}.
While differences in model used, prompt wording, and exact city names used may account for some variation, we hypothesize that the large discrepancy in reasoning ability between pairwise (two-way) and three-way directional relations is an indication of a fundamental lack of spatial reasoning ability in current LLMs.





\subsection{Result 4. Topological Relations}
\paragraph{Results and Discussion}
The returned results for the topological relation prompts were correct in 13 out of 18 cases.
We hypothesize that the better performance on topological relations than cyclic order or three-way directional relations is due to two factors.
The first factor is the qualitative nature of topological relations, which lend themselves well to natural language descriptions, making them more prevalent (at least explicitly) in LLM training data.
The second factor is that this test only involved two entities per prompt, which makes it more likely that the information required to answer the question may have been seen explicitly at training time, reducing the need for the model to perform spatial reasoning to answer the question correctly.
%To disentangle these factors, further investigation could be done testing complex topological relations between more than two entities.



\subsection{Result 5. Order Relations}
The returned results for the cyclic order relation prompts were correct in 0 out of 18 cases.
We observed that many of the responses indicated a lack of knowledge about relative positions of cities.
For example, one output was 
``Without specific information about the relative positions of Fraser Island, Alice Springs, and Albury-Wodonga, it's challenging to provide an accurate clockwise ordering.''
In the context of multiple previous studies that have shown Chat-GPT and other LLMs can successfully provide geocoordinates of common cities and well-known places~\cite{Bhandari2023,Qi2023}, we hypothesize that LLMs may not be able to make the jump from absolute to relative position. 










% Report percent of 2 hop spatial queries correct.
% Report percent of space + time queries correct.
% Report for more hops if tested.